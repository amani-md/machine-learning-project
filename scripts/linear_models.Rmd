---
title: "linear_models"
output: html_document
date: "2025-10-31"
---


```{r include=FALSE}
## libraries
packages <- c("here", "caret", "glmnet", "dplyr", "tidyr", "ggplot2")

installed <- packages %in% rownames(installed.packages())
if (any(!installed)) {
  install.packages(packages[!installed])
}

lapply(packages, library, character.only = TRUE)
```

```{r include=FALSE}
# import 
data_path <- here("data", "restaurant_revenue.csv")

if (!file.exists(data_path)) {
  stop("File not found : ", data_path)
}

restaurant_revenue <- read.csv(data_path)
```


## Data Visualisation & Manipulation

```{r}
# transforming categorical variables into numerical variables
## American = 0, Italian = 1, Japanese = 2, Mexican = 3

restaurant_revenue$Cuisine_Type[restaurant_revenue$Cuisine_Type == "American"] <- 0
restaurant_revenue$Cuisine_Type[restaurant_revenue$Cuisine_Type == "Italian"] <- 1
restaurant_revenue$Cuisine_Type[restaurant_revenue$Cuisine_Type == "Japanese"] <- 2
restaurant_revenue$Cuisine_Type[restaurant_revenue$Cuisine_Type == "Mexican"] <- 3

```

```{r}
set.seed(123)
# split train / test subsets 
train_index = createDataPartition(restaurant_revenue$Monthly_Revenue, p = 0.7, list = FALSE)
train_data = restaurant_revenue[train_index, ]
test_data = restaurant_revenue[-train_index, ]
```

## Linear regression

The aim is to build and compare linear regression models for predicting **restaurant monthly revenue** using stepwise selection methods based on the Bayesian Information Criterion (BIC)

```{r}
# fit linear model
full_model <- lm(Monthly_Revenue ~ . , data = train_data)
```

```{r}
# forward stepwise selection with BIC
empty_model <- lm(Monthly_Revenue ~ 1, data = train_data)

forward_model <- step(empty_model,
                      scope = list(lower = empty_model, upper = full_model),
                      direction = "forward",
                      k = log(nrow(train_data)),
                      trace = FALSE)
```

```{r}
# backward stepwise selection with BIC
backward_model <- step(full_model,
                       direction = "backward",
                       k = log(nrow(train_data)),
                       trace = FALSE)
```

```{r}
# comparison

## selected variables for each model 
selected_full <- names(coef(full_model))[-1]
selected_forward <- names(coef(forward_model))[-1]
selected_backward <- names(coef(backward_model))[-1]

# adjusted R2 for each model
adj_r2_full <- summary(full_model)$adj.r.squared
adj_r2_forward <- summary(forward_model)$adj.r.squared
adj_r2_backward <- summary(backward_model)$adj.r.squared

regression_table <- 
  data.frame(
    Model = c("Forward Selection", "Backward Selection", "Linear Model"),
    Predictors = sapply(list(selected_forward, selected_backward, selected_full), paste, collapse = ", "),
    Adjusted_R2 = c(adj_r2_forward, adj_r2_backward, adj_r2_full)
  )

regression_table
```

**Observations:** essentially, adjusted $R^2$ is pretty much the same accross the three models. Plus, both forward/backward end up with the same linear model which is *Monthly_Revenue \~ Number_of_Customers + Menu_Price + Marketing_Spend*.

## Ridge regression

The goal is to examine how Ridge and Lasso regression techniques apply regularization and how the model coefficients behave across varying levels of penalty strength.

```{r}
# prep 
set.seed(123)
## matrix 
X <- as.matrix(train_data[, c("Number_of_Customers", "Menu_Price", "Marketing_Spend", "Average_Customer_Spending", "Reviews")])
y <- train_data$Monthly_Revenue
```

```{r}
# ridge regression
ridge_fit = glmnet(X, y, alpha=0, standardize = TRUE)
```

```{r}
# coeff evolution depending on the value of the tuning parameter lambda 

## coefficients extraction
coef_matrix <- as.matrix(ridge_fit$beta)
ridge_df <- as.data.frame(t(coef_matrix))
ridge_df$log_lambda <- log10(ridge_fit$lambda)
ridge_df <- pivot_longer(ridge_df, -log_lambda, names_to = "Predictor", values_to = "Coefficient")

## plot
ggplot(ridge_df, aes(x = log_lambda, y = Coefficient, color = Predictor)) +
  geom_line(linewidth = 1) +
  labs(title = "Ridge Coefficient Paths", x = "log10(λ)", y = "Coefficient") +
  theme_bw() 
```

```{r}
# CV to choose the best lambda
cv_ridge = cv.glmnet(X, y, alpha = 0, standardize = TRUE)
cat("λ with the smallest MSE:", cv_ridge$lambda.min, "\n")
```

```{r}
# plot MSE over lambda
plot(cv_ridge) 

log_lambda_vals <- log(cv_ridge$lambda)
mean_error_vals <- cv_ridge$cvm
points(log_lambda_vals, mean_error_vals, pch = 19, col = "#6bafd6", cex = 0.8)

abline(v = log(cv_ridge$lambda.min), lty = 2, col = "red")
abline(v = log(cv_ridge$lambda.1se), lty = 2, col = "blue")

legend(
  "bottomright",
  legend = c(
    expression(lambda["min"] * ": Minimum MSE"), 
    expression(lambda["1se"] * ": 1-Standard Error Rule")
  ),
  col = c("red", "blue"),
  lty = c(2, 2),
  lwd = 1,
  cex = 0.8,
  bty = "n"
  )
```

```{r}
# test matrices 
X1 <- as.matrix(test_data[, c("Number_of_Customers", "Menu_Price", "Marketing_Spend", 
                              "Average_Customer_Spending", "Reviews")])
y1 <- test_data$Monthly_Revenue
```

```{r}
# predict 
y_predicted <- predict(cv_ridge, s = "lambda.min", newx = X1)

# computing R2
r2 <- 1 - (sum((y1 - y_predicted)^2)) / (sum((y1 - mean(y1))^2))
mse <- mean((y1 - y_predicted)^2)
rmse <- sqrt(mse)

cat("R2 of ridge regression model:", r2, "\n")
cat("MSE of ridge regression model:", mse, "\n")
cat("RMSE of ridge regression model:", rmse, "\n")
```

## Lasso regression

```{r}
# fit lasso regression model 
lasso_fit <- glmnet(X, y, alpha = 1, standardize = TRUE)
```

```{r}
# coefficients extraction
coef_matrix <- as.matrix(lasso_fit$beta)
lasso_df <- as.data.frame(t(coef_matrix))
lasso_df$log_lambda <- log10(lasso_fit$lambda)
lasso_df <- pivot_longer(lasso_df, -log_lambda, names_to = "Predictor", values_to = "Coefficient")

# plot
ggplot(lasso_df, aes(x = log_lambda, y = Coefficient, color = Predictor)) +
  geom_line(linewidth = 1) +
  labs(title = "Lasso Coefficient Paths", x = "log10(λ)", y = "Coefficient") +
  theme_bw()
```

```{r}
# CV to choose the best lambda
cv_lasso = cv.glmnet(X, y, alpha = 1, standardize = TRUE) #best model as output
cat("λ with the smallest MSE:", cv_lasso$lambda.min, "\n")
```

```{r}
# plot MSE over lambda
plot(cv_lasso) 

log_lambda_vals <- log(cv_lasso$lambda)
mean_error_vals <- cv_lasso$cvm
points(log_lambda_vals, mean_error_vals, pch = 19, col = "#6bafd6", cex = 0.8)

abline(v = log(cv_lasso$lambda.min), lty = 2, col = "red")
abline(v = log(cv_lasso$lambda.1se), lty = 2, col = "blue")

legend(
  "topleft",
  legend = c(
    expression(lambda["min"] * ": Minimum MSE"), 
    expression(lambda["1se"] * ": 1-Standard Error Rule")
  ),
  col = c("red", "blue"),
  lty = c(2, 2),
  lwd = 1,
  cex = 0.8,
  bty = "n"
  )
```

```{r}
# MSE over lambda - ADD LEGEND & SECOND X AXIS 
n_nonzero <- apply(coef(cv_lasso$glmnet.fit)[-1, ], 2, function(x) sum(x != 0))

cv_df <- data_frame(
  lambda = cv_lasso$lambda,
  mean = cv_lasso$cvm,
  se = cv_lasso$cvsd,
  log_lambda = log(cv_lasso$lambda),
  n_nonzero = n_nonzero)

ggplot(cv_df, aes(x = log_lambda, y = mean)) +
  geom_point(size = 2, color = "#6bafd6") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.05, color = "grey50") +
  geom_vline(xintercept = log(cv_lasso$lambda.min), linetype = "dashed", color = "red") +
  geom_vline(xintercept = log(cv_lasso$lambda.1se), linetype = "dashed", color = "blue") +
  labs(
    title = "Cross-Validation Curve",
    x = "Log(λ)",
    y = "Mean Squared Error") + 
  theme_bw()
```

```{r}
# predict 
y_predicted <- predict(cv_lasso, s = "lambda.min", newx = X1)

# computing R2
r2 <- 1 - (sum((y1 - y_predicted)^2)) / (sum((y1 - mean(y1))^2))
mse <- mean((y1 - y_predicted)^2)
rmse <- sqrt(mse)

cat("R2 of lasso regression model:", r2, "\n")
cat("MSE of lasso regression model:", mse, "\n")
cat("RMSE of lasso regression model:", rmse, "\n")
```

## Principal Components Regression & Partial Least Squares Regression

Repenser cette section

```{r}
library(pls)
pcr_model <- pcr(y ~ X, validation = "CV", ncomp = 5, scale = TRUE, segments = 10)
mse_pcr <- MSEP(pcr_model)$val[1, 1, -1]
```

```{r}
# plsr() function is analogous to the pcr() function. It returns object of class "plsr". 
pls_model <- plsr(y ~ X, validation = "CV", ncomp = 5, scale = TRUE, segments = 10)

# Extract cross-validated MSEs from the PLS model for components 1 to 6 
mse_pls <- MSEP(pls_model)$val[1, 1, -1]
```

```{r}
results_df <- tibble(
  Components = 1:5,
  PCR = mse_pcr,
  PLS = mse_pls
) %>%
  pivot_longer(-Components, names_to = "Method", values_to = "MSE")

ggplot(results_df, aes(x = Components, y = MSE, color = Method)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "CV MSE VS Number of Components", x = "Number of Components", y = "10-fold CV MSE") +
  theme_bw()
```

```{r}
opt_pcr <- which.min(mse_pcr)
opt_pls <- which.min(mse_pls)

summary_df <- data.frame(
  Method = c("PCR", "PLS"),
  Optimal_Components = c(opt_pcr, opt_pls),
  Test_MSE = c(mse_pcr[opt_pcr], mse_pls[opt_pls])
)
summary_df
```
